import pdb
from pprint import pprint
import re
import numpy as np
from copy import deepcopy

import jax.numpy as jnp

from utils import all_pairs_str, NON_GC_PAIRS
from utils import CELL_TEMP, MAX_LOOP

from jax.config import config
config.update("jax_enable_x64", True)


def precompute_initiation(init_table, n):
    precomputed = np.empty(n, dtype=np.float64)
    for u in range(MAX_LOOP):
        precomputed[u] = init_table[u]
    for u in range(MAX_LOOP, n):
        # extrapolated = init_table[MAX_LOOP] + np.round(1.07856 * np.log(u/MAX_LOOP), 2)
        extrapolated = init_table[MAX_LOOP] + np.floor(1.07856 * np.log(u/MAX_LOOP) * 100) / 100
        precomputed[u] = extrapolated
    return jnp.array(precomputed)


def process_stacking(stack_lines):
    pairs = ['CG', 'GC', 'GU', 'UG', 'AU', 'UA', 'NN'] # ordering in .par file
    n_pairs = len(pairs)
    stack_data = {p: dict() for p in pairs}
    stack_lines = np.array([l.split() for l in stack_lines], dtype=np.float64) # split and cast to float

    # FIXME: 5'->3' directionality is unclear in the Vienna .par files. Doesn't align with NNDB as it is now.
    for i, p1 in zip(range(n_pairs), pairs): # FIXME: could just `enumerate(pairs)`
        for j, p2 in zip(range(n_pairs), pairs):
            stack_data[p1][p2] = stack_lines[i][j] / 100

    return stack_data

def process_int11(int11_lines):
    pairs = ['CG', 'GC', 'GU', 'UG', 'AU', 'UA', 'NN'] # ordering in .par file
    nucs = ['N', 'A', 'C', 'G', 'U'] # ordering in .par file
    n_pairs = len(pairs)
    int11_data = dict()
    int11_lines = np.array([l.split() for l in int11_lines], dtype=np.float64) # split and cast to float
    for n_p1, p1 in enumerate(pairs):
        p1_start_idx = len(pairs) * len(nucs) * n_p1 # the first line where the first pair is p1
        int11_data[p1] = dict()
        for n_p2, p2 in enumerate(pairs):
            p1_p2_start_idx = p1_start_idx + len(nucs) * n_p2 # the first line where the first pair is p1 and the second pair is p2
            int11_data[p1][p2] = dict()
            for nuc1_idx, nuc1 in enumerate(nucs):
                p1_p2_nuc1_line_idx = p1_p2_start_idx + nuc1_idx # the line with pair1,pair2,nuc1
                p1_p2_nuc1_line = int11_lines[p1_p2_nuc1_line_idx]
                int11_data[p1][p2][nuc1] = dict()
                for nuc2_idx, nuc2 in enumerate(nucs):
                    int11_data[p1][p2][nuc1][nuc2] = p1_p2_nuc1_line[nuc2_idx] / 100
    return int11_data

def process_int21(int21_lines):
    pairs = ['CG', 'GC', 'GU', 'UG', 'AU', 'UA', 'NN'] # ordering in .par file
    nucs = ['N', 'A', 'C', 'G', 'U'] # ordering in .par file
    n_pairs = len(pairs)
    int21_data = dict()
    int21_lines = np.array([l.split() for l in int21_lines], dtype=np.float64) # split and cast to float
    for n_p1, p1 in enumerate(pairs):
        p1_start_idx = len(pairs) * len(nucs)**2 * n_p1
        int21_data[p1] = dict()
        for n_p2, p2 in enumerate(pairs):
            p1_p2_start_idx = p1_start_idx + len(nucs)**2 * n_p2
            int21_data[p1][p2] = dict()
            for nuc1_idx, nuc1 in enumerate(nucs):
                for nuc2_idx, nuc2 in enumerate(nucs):
                    # To be explicit, we set the key as the pair of nucleotides (i.e. the `2` in the 2x1)
                    n1n2 = nuc1 + nuc2
                    int21_data[p1][p2][n1n2] = dict()
                    p1_p2_nuc12_line_idx = p1_p2_start_idx + nuc1_idx * len(nucs) + nuc2_idx # nuc12 is shorthand for nuc1_nuc2
                    p1_p2_nuc12_line = int21_lines[p1_p2_nuc12_line_idx]
                    for nuc3_idx, nuc3 in enumerate(nucs):
                        int21_data[p1][p2][n1n2][nuc3] = p1_p2_nuc12_line[nuc3_idx] / 100
    return int21_data

def process_int22(int22_lines):
    pairs = ['CG', 'GC', 'GU', 'UG', 'AU', 'UA'] # ordering in .par file
    nucs = ['A', 'C', 'G', 'U'] # ordering in .par file
    n_pairs = len(pairs)
    int22_data = dict()
    int22_lines = np.array([l.split() for l in int22_lines], dtype=np.float64) # split and cast to float
    for n_p1, p1 in enumerate(pairs):
        p1_start_idx = len(pairs) * len(nucs)**3 * n_p1
        int22_data[p1] = dict()
        for n_p2, p2 in enumerate(pairs):
            p1_p2_start_idx = p1_start_idx + len(nucs)**3 * n_p2
            int22_data[p1][p2] = dict()
            for nuc1_idx, nuc1 in enumerate(nucs):
                for nuc2_idx, nuc2 in enumerate(nucs):
                    # To be explicit, we set the keys as the pairs of nucleotides (i.e. the `2`s in the 2x2)
                    n1n2 = nuc1 + nuc2
                    int22_data[p1][p2][n1n2] = dict()
                    for nuc3_idx, nuc3 in enumerate(nucs):
                        p1_p2_n12_n3_line_idx = p1_p2_start_idx + nuc1_idx * len(nucs)**2 + nuc2_idx * len(nucs) + nuc3_idx
                        p1_p2_n12_n3_line = int22_lines[p1_p2_n12_n3_line_idx]
                        for nuc4_idx, nuc4 in enumerate(nucs):
                            int22_data[p1][p2][n1n2][nuc3 + nuc4] = p1_p2_n12_n3_line[nuc4_idx] / 100
    return int22_data

# process mismatches for internal loops, multiloops, and exterior loops
def process_int_multi_ext_mismatches(mismatch_lines):
    mismatch_lines = np.array([l.split() for l in mismatch_lines], dtype=np.float64)
    pairs = ['CG', 'GC', 'GU', 'UG', 'AU', 'UA'] # ordering in .par file. note: ignoring NS
    mismatch_data = {p: dict() for p in pairs}
    nucs = ['N', 'A', 'C', 'G', 'U']
    for i, pair in enumerate(pairs):
        pair_start_idx = i * len(nucs)
        for nuc1_idx, nuc1 in enumerate(nucs):
            pair_n_line = mismatch_lines[pair_start_idx + nuc1_idx]
            for nuc2_idx, nuc2 in enumerate(nucs):
                # note: we save data with pairs as keys
                mismatch_data[pair][nuc1 + nuc2] = pair_n_line[nuc2_idx] / 100
    return mismatch_data


# https://www.tbi.univie.ac.at/RNA/ViennaRNA/doc/html/1_88_84__epars_8h_source.html
def process_mismatch_hairpin(mismatch_hairpin_lines):
    mismatch_hairpin_lines = np.array([l.split() for l in mismatch_hairpin_lines], dtype=np.float64)
    pairs = ['CG', 'GC', 'GU', 'UG', 'AU', 'UA'] # ordering in .par file
    mismatch_hairpin_data = {p: dict() for p in pairs}
    for i, pair in enumerate(pairs):
        pair_start_idx = i*5
        pair_lines = mismatch_hairpin_lines[pair_start_idx:pair_start_idx+5]
        # loop per ordering in .par file
        for l, nuc1 in zip(pair_lines, ['N', 'A', 'C', 'G', 'U']):
            for val, nuc2 in zip(l, ['N', 'A', 'C', 'G', 'U']):
                mismatch_hairpin_data[pair][nuc1 + nuc2] = val / 100
    return mismatch_hairpin_data

def process_dangle(dangle_lines):
    dangle_lines = np.array([l.split() for l in dangle_lines], dtype=np.float64)
    pairs = ['CG', 'GC', 'GU', 'UG', 'AU', 'UA'] # ordering in .par file
    dangle_data = {p: dict() for p in pairs}
    for i, pair in enumerate(pairs):
        for j, nuc in enumerate(['N', 'A', 'C', 'G', 'U']):
            dangle_data[pair][nuc] = dangle_lines[i][j] / 100
    return dangle_data


def postprocess_interior_mismatch(raw_interior_mismatch_data):
    # Note: our flattened version represents the ordering if we take the tensor product of
    # (mismatch 1 possibilities) and (mismatch 2 possibilities)
    nucs = ['A', 'C', 'G', 'U']
    flattened_int_mismatch_data = np.full((len(all_pairs_str) * len(nucs)**2,), np.nan, dtype=np.float64)
    idx = 0
    for p1 in all_pairs_str:
        for n1 in nucs:
            for n2 in nucs:
                flattened_int_mismatch_data[idx] = raw_interior_mismatch_data[p1][n1+n2]
                idx += 1

    return flattened_int_mismatch_data
    """
    # We take a kronecker *sum*, not product -- https://mathworld.wolfram.com/KroneckerSum.html
    flattened_int_mismatch_data = np.kron(flattened_int_mismatch_data, np.ones(idx)) \
                                  + np.kron(np.ones(idx), flattened_int_mismatch_data)
    return flattened_int_mismatch_data
    """

def postprocess(data, max_precompute, default_stack_energy=1e6):
    nucs = ['A', 'C', 'G', 'U']
    postprocessed_data = deepcopy(data)

    # Stack
    raw_stack_data = data['stack']
    flattened_stack_data = np.full((len(all_pairs_str)**2,), np.nan, dtype=np.float64)

    idx = 0
    for p1 in all_pairs_str:
        for p2 in all_pairs_str:
            p1_p2_dg = raw_stack_data[p1][p2]
            flattened_stack_data[idx] = p1_p2_dg
            idx += 1
    postprocessed_data['stack'] = flattened_stack_data

    # Mismatch hairpin
    raw_mis_hairpin_data = data['mismatch_hairpin']
    flattened_mis_hairpin_data = np.empty((16*len(all_pairs_str),), dtype=np.float64)
    idx = 0
    for pair in all_pairs_str:
        for n1 in nucs:
            for n2 in nucs:
                flattened_mis_hairpin_data[idx] = raw_mis_hairpin_data[pair][n1 + n2]
                idx += 1
    postprocessed_data['mismatch_hairpin'] = flattened_mis_hairpin_data

    # int11
    raw_int11_data = data['int11']
    n_int11 = len(all_pairs_str) * len(all_pairs_str) * len(nucs) * len(nucs)
    flattened_int11_data = np.full((n_int11,), np.nan, dtype=np.float64)
    idx = 0
    for pair1 in all_pairs_str:
        for pair2 in all_pairs_str:
            for n1 in nucs:
                for n2 in nucs:
                    flattened_int11_data[idx] = raw_int11_data[pair1][pair2][n1][n2]
                    idx += 1
    postprocessed_data['int11'] = flattened_int11_data

    # int21
    raw_int21_data = data['int21']
    n_int21 = len(all_pairs_str)**2 * len(nucs)**3
    flattened_int21_data = np.full((n_int21,), np.nan, dtype=np.float64)
    idx = 0
    for pair1 in all_pairs_str:
        for pair2 in all_pairs_str:
            for n1 in nucs:
                for n2 in nucs:
                    n1n2 = n1 + n2
                    for n3 in nucs:
                        flattened_int21_data[idx] = raw_int21_data[pair1][pair2][n1n2][n3]
                        idx += 1
    postprocessed_data['int21'] = flattened_int21_data

    # int22
    raw_int22_data = data['int22']
    n_int22 = len(all_pairs_str)**2 * len(nucs)**4
    flattened_int22_data = np.full((n_int22,), np.nan, dtype=np.float64)
    idx = 0
    for pair1 in all_pairs_str:
        for pair2 in all_pairs_str:
            for n1 in nucs:
                for n2 in nucs:
                    n1n2 = n1 + n2
                    for n3 in nucs:
                        for n4 in nucs:
                            n3n4 = n3 + n4
                            flattened_int22_data[idx] = raw_int22_data[pair1][pair2][n1n2][n3n4]
                            idx += 1
    postprocessed_data['int22'] = flattened_int22_data

    # interior mismatches
    postprocessed_data['mismatch_interior'] = postprocess_interior_mismatch(data['mismatch_interior'])
    postprocessed_data['mismatch_interior_1n'] = postprocess_interior_mismatch(data['mismatch_interior_1n'])
    postprocessed_data['mismatch_interior_23'] = postprocess_interior_mismatch(data['mismatch_interior_23'])

    # multiloop mismatches
    raw_mismatch_multi_data = data['mismatch_multi']
    flattened_mismatch_multi_data = np.empty((len(all_pairs_str)*16,), dtype=np.float64)
    idx = 0
    for pair in all_pairs_str:
        for n1 in nucs:
            for n2 in nucs:
                flattened_mismatch_multi_data[idx] = raw_mismatch_multi_data[pair][n1 + n2]
                idx += 1
    postprocessed_data['mismatch_multi'] = flattened_mismatch_multi_data


    # exterior mismatches
    raw_mismatch_exterior_data = data['mismatch_exterior']
    flattened_mismatch_exterior_data = np.empty((len(all_pairs_str)*16,), dtype=np.float64)
    idx = 0
    for pair in all_pairs_str:
        for n1 in nucs:
            for n2 in nucs:
                flattened_mismatch_exterior_data[idx] = raw_mismatch_exterior_data[pair][n1 + n2]
                idx += 1
    postprocessed_data['mismatch_exterior'] = flattened_mismatch_exterior_data

    # dangle5 and dangle3
    raw_dangle5_data = data['dangle5']
    flattened_dangle5_data = np.empty((len(all_pairs_str)*4,), dtype=np.float64)
    idx = 0
    for pair in all_pairs_str:
        for n in nucs:
            flattened_dangle5_data[idx] = raw_dangle5_data[pair][n]
            idx += 1
    postprocessed_data['dangle5'] = flattened_dangle5_data

    raw_dangle3_data = data['dangle3']
    flattened_dangle3_data = np.empty((len(all_pairs_str)*4,), dtype=np.float64)
    idx = 0
    for pair in all_pairs_str:
        for n in nucs:
            flattened_dangle3_data[idx] = raw_dangle3_data[pair][n]
            idx += 1
    postprocessed_data['dangle3'] = flattened_dangle3_data

    # non-GC closing penalty -- vectorize for compatability with 2-loops
    non_gc_closing_penalty_two_loops = np.empty((len(all_pairs_str)**2,), dtype=np.float64)
    idx = 0
    for p1 in all_pairs_str:
        p1_penalty = data['non_gc_closing_penalty'] if p1 in NON_GC_PAIRS else 0.0
        for p2 in all_pairs_str:
            p2_penalty = data['non_gc_closing_penalty'] if p2 in NON_GC_PAIRS else 0.0
            non_gc_closing_penalty_two_loops[idx] = p1_penalty + p2_penalty
            idx += 1
    postprocessed_data['non_gc_closing_two_loop'] = non_gc_closing_penalty_two_loops

    # asymmetry
    asymmetry_matrix = np.empty((max_precompute, max_precompute), dtype=np.float64)
    for n1 in range(max_precompute):
        for n2 in range(max_precompute):
            n1_n2_abs = np.abs(n1 - n2)
            n1_n2_dg = data['asymmetry'] * n1_n2_abs
            n1_n2_dg = np.min([n1_n2_dg, data['asymmetry_max']])
            asymmetry_matrix[n1, n2] = n1_n2_dg
    postprocessed_data['asymmetry_matrix'] = jnp.array(asymmetry_matrix)


    return postprocessed_data

def read(param_path="../misc/rna_turner2004.par", max_precompute=200):
    with open(param_path, 'r') as f:
        # param_text = f.read()
        param_lines = f.readlines()

    # read in chunks
    chunks = list()
    next_chunk = list()
    INF = str(1e6)
    for l in param_lines:
        if l.isspace():
            # skip lines that are only whitespace
            continue

        if l[0] == "#":
            chunks.append(next_chunk)
            next_chunk = list()
        processed_line = re.sub("/\*.*\*/", "", l).strip() # remove comments
        processed_line = processed_line.replace("INF", INF) # set inf to 1e6
        next_chunk.append(processed_line)
    chunks.append(next_chunk) # do the last one

    assert(not chunks[0]) # first one should be empty
    assert(len(chunks[1]) == 1 and chunks[1][0] == '## RNAfold parameter file v2.0')
    chunks = chunks[2:]

    # process the chunks we care about
    # store in an easy-to-manipulate form
    data = dict()
    for chunk in chunks:
        first_line = chunk[0]
        assert(first_line[0] == '#')

        chunk_name = first_line.split('#')[-1].strip()
        if chunk_name == "stack":
            chunk_data = chunk[2:]
            stack_data = process_stacking(chunk_data)
            data['stack'] = stack_data
        elif chunk_name == "bulge":
            chunk_data = chunk[1:]
            bulge_initiations = np.array(' '.join(chunk_data).split(), dtype=np.float64) # note: 0th index is for n=0
            bulge_initiations /= 100
            data['bulge'] = precompute_initiation(bulge_initiations, max_precompute)
        elif chunk_name == "hairpin":
            chunk_data = chunk[1:]
            hairpin_initiations = np.array(' '.join(chunk_data).split(), dtype=np.float64) # note: 0th index is for n=0
            hairpin_initiations /= 100
            data['hairpin'] = precompute_initiation(hairpin_initiations, max_precompute)
        elif chunk_name == "mismatch_hairpin":
            chunk_data = chunk[1:]
            mismatch_hairpin_data =  process_mismatch_hairpin(chunk_data)
            data['mismatch_hairpin'] = mismatch_hairpin_data
        elif chunk_name == "interior":
            chunk_data = chunk[1:]
            interior_initiations = np.array(' '.join(chunk_data).split(), dtype=np.float64) # note: 0th index is for n=0
            interior_initiations /= 100
            data['interior'] = precompute_initiation(interior_initiations, max_precompute)
        elif chunk_name == "NINIO":
            chunk_data = chunk[1:]
            assert(len(chunk_data) == 1)
            ninio_data = np.array(chunk_data[0].split(), dtype=np.float64)
            data['asymmetry'] = ninio_data[0] / 100
            data['asymmetry_max'] = ninio_data[2] / 100
        elif chunk_name == "Misc":
            chunk_data = chunk[1:]
            assert(len(chunk_data) == 1)
            misc_data = np.array(chunk_data[0].split(), dtype=np.float64)
            data['duplex_init'] = misc_data[0] / 100
            data['non_gc_closing_penalty'] = misc_data[2] / 100
        elif chunk_name == "ML_params":
            chunk_data = chunk[1:]
            assert(len(chunk_data) == 1)
            ml_params_data = np.array(chunk_data[0].split(), dtype=np.float64)
            data['ml_unpaired'] = ml_params_data[0] / 100
            data['ml_initiation'] = ml_params_data[2] / 100
            data['ml_branch'] = ml_params_data[4] / 100
        elif chunk_name == "int11":
            chunk_data = chunk[1:]
            int11_data = process_int11(chunk_data)
            data['int11'] = int11_data
        elif chunk_name == "int21":
            chunk_data = chunk[1:]
            int21_data = process_int21(chunk_data)
            data['int21'] = int21_data
        elif chunk_name == "int22":
            chunk_data = chunk[1:]
            int22_data = process_int22(chunk_data)
            data['int22'] = int22_data
        elif chunk_name in ["mismatch_interior", "mismatch_interior_1n", "mismatch_interior_23"]:
            chunk_data = chunk[1:]
            mismatch_int_data = process_int_multi_ext_mismatches(chunk_data)
            data[chunk_name] = mismatch_int_data
        elif chunk_name == "mismatch_multi":
            chunk_data = chunk[1:]
            mismatch_multi_data = process_int_multi_ext_mismatches(chunk_data)
            data['mismatch_multi'] = mismatch_multi_data
        elif chunk_name == "mismatch_exterior":
            chunk_data = chunk[1:]
            mismatch_ext_data = process_int_multi_ext_mismatches(chunk_data)
            data['mismatch_exterior'] = mismatch_ext_data
        elif chunk_name in ["dangle5", "dangle3"]:
            chunk_data = chunk[2:]
            dangle_data = process_dangle(chunk_data)
            data[chunk_name] = dangle_data
        elif chunk_name in ["Hexaloops", "Tetraloops", "Triloops"]:
            chunk_name = chunk_name.lower()
            chunk_data = chunk[1:]
            special_hairpin_map = dict()
            for case in chunk_data:
                seq, dg, dh = case.split()
                special_hairpin_map[seq] = float(dg) / 100
            data[chunk_name] = special_hairpin_map



    # post process (subject to change)
    return data, postprocess(data, max_precompute)




if __name__ == "__main__":
    params_dir = "../misc"
    params_fname = "rna_turner2004.par"
    # params_fname = "rna_turner1999.par"
    params_path = f"{params_dir}/{params_fname}"
    read(params_path)
